{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-adeleke1/Association_of_Data_Scientists/blob/main/Google_A2A_Project_Create_OpenAI_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pul8NUYin7Tq"
      },
      "source": [
        "# Google A2A â†’ OpenAI Agent\n",
        "This notebook installs the A2A SDK, creates an OpenAI-powered agent, launches an A2A server in the background, and tests it via HTTP.\n"
      ],
      "id": "Pul8NUYin7Tq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900c5c6a-1b75-4660-cf34-c08d588b2c6e"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip -q install a2a-sdk \"openai>=1.59.0\" langgraph langchain-openai uvicorn httpx nest_asyncio\n",
        "print('Installed a2a-sdk, openai, langgraph, langchain-openai, uvicorn, httpx, nest_asyncio')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed a2a-sdk, openai, langgraph, langchain-openai, uvicorn, httpx, nest_asyncio\n"
          ]
        }
      ],
      "execution_count": 9,
      "id": "install_deps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env_and_asyncio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf6d00d-18d6-46e6-8d6e-40b207225d80"
      },
      "source": [
        "import os, nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set your API key here or rely on an already-set environment variable\n",
        "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "# print('OPENAI_API_KEY present:', bool(os.getenv('OPENAI_API_KEY')))\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "key = userdata.get(\"OPENAI_API_KEY\")     # exact name match, case-sensitive\n",
        "print(\"Secret found:\", bool(key))\n",
        "\n",
        "if key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = key.strip()  # strip just in case\n",
        "    print(\"Env set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
        "else:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not granted to this notebook in Secrets.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secret found: True\n",
            "Env set: True\n"
          ]
        }
      ],
      "execution_count": 13,
      "id": "env_and_asyncio"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_pkg_dir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dcf3a0-a270-433c-911e-24108780d3a4"
      },
      "source": [
        "import os, textwrap, pathlib\n",
        "base_dir = '/content/my_project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "print('Project dir:', base_dir)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project dir: /content/my_project\n"
          ]
        }
      ],
      "execution_count": 14,
      "id": "create_pkg_dir"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "write_agent_py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578a6cb5-fc0e-4d9e-a6b7-f764548df2bf"
      },
      "source": [
        "agent_py = '''\\\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "@dataclass\n",
        "class OpenAIAgentConfig:\n",
        "    model: str = \"gpt-4o-mini\"\n",
        "    temperature: float = 0.2\n",
        "\n",
        "def create_openai_agent(openai_model: str, temperature: float = 0.2) -> OpenAIAgentConfig:\n",
        "    \"\"\"Create a tiny config object describing how to call OpenAI.\"\"\"\n",
        "    return OpenAIAgentConfig(model=openai_model, temperature=temperature)\n",
        "\n",
        "async def run_openai(agent: OpenAIAgentConfig, prompt: str) -> str:\n",
        "    \"\"\"Call OpenAI Chat API and return the assistant's text.\"\"\"\n",
        "    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    resp = await client.chat.completions.create(\n",
        "        model=agent.model,\n",
        "        temperature=agent.temperature,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return resp.choices[0].message.content or \"\"\n",
        "'''\n",
        "open(os.path.join(base_dir, 'agent.py'), 'w').write(agent_py)\n",
        "print('Wrote', os.path.join(base_dir, 'agent.py'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/my_project/agent.py\n"
          ]
        }
      ],
      "execution_count": 15,
      "id": "write_agent_py"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "write_task_manager_py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9405604e-540a-4115-d1b0-97e406162fd7"
      },
      "source": [
        "task_manager_py = '''\\\n",
        "from __future__ import annotations\n",
        "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
        "from a2a.server.events import EventQueue\n",
        "from a2a.utils import new_agent_text_message\n",
        "from my_project.agent import create_openai_agent, run_openai\n",
        "\n",
        "def _message_text(context: RequestContext) -> str:\n",
        "    \"\"\"\n",
        "    Extract plain text from the incoming A2A message.\n",
        "    Falls back gracefully if parts are non-text.\n",
        "    \"\"\"\n",
        "    if not context.message or not getattr(context.message, \"parts\", None):\n",
        "        return \"\"\n",
        "    texts: list[str] = []\n",
        "    for part in context.message.parts:\n",
        "        text = getattr(part, \"text\", None)\n",
        "        if text is None and hasattr(part, \"root\"):\n",
        "            text = getattr(part.root, \"text\", None)\n",
        "        if isinstance(text, str):\n",
        "            texts.append(text)\n",
        "    return \"\\\\n\".join(texts).strip()\n",
        "\n",
        "class MyAgentTaskManager(AgentExecutor):\n",
        "    \"\"\"\n",
        "    An AgentExecutor that routes requests to OpenAI.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_model: str = \"gpt-4o-mini\", temperature: float = 0.2):\n",
        "        self._cfg = create_openai_agent(openai_model, temperature)\n",
        "\n",
        "    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
        "        user_text = _message_text(context) or \"Please respond helpfully.\"\n",
        "        reply = await run_openai(self._cfg, user_text)\n",
        "        await event_queue.enqueue_event(\n",
        "            new_agent_text_message(reply, context_id=context.context_id)\n",
        "        )\n",
        "\n",
        "    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
        "        await event_queue.enqueue_event(\n",
        "            new_agent_text_message(\"Cancellation is not supported for this agent.\", context_id=context.context_id)\n",
        "        )\n",
        "'''\n",
        "open(os.path.join(base_dir, 'task_manager.py'), 'w').write(task_manager_py)\n",
        "print('Wrote', os.path.join(base_dir, 'task_manager.py'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/my_project/task_manager.py\n"
          ]
        }
      ],
      "execution_count": 16,
      "id": "write_task_manager_py"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "write_init_py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54eccbbe-9a1b-4b7f-d297-3928a387689d"
      },
      "source": [
        "init_py = '''\\\n",
        "import argparse\n",
        "import os\n",
        "import uvicorn\n",
        "from a2a.server.apps import A2AStarletteApplication\n",
        "from a2a.server.request_handlers import DefaultRequestHandler\n",
        "from a2a.server.tasks import InMemoryTaskStore\n",
        "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
        "from my_project.task_manager import MyAgentTaskManager\n",
        "\n",
        "def main(host: str = \"0.0.0.0\", port: int = 9999, openai_model: str = \"gpt-4o-mini\"):\n",
        "    skill = AgentSkill(\n",
        "        id=\"general_chat\",\n",
        "        name=\"General Chat\",\n",
        "        description=\"Answers questions with helpful, concise responses.\",\n",
        "        tags=[\"chat\", \"assistant\"],\n",
        "        examples=[\"Explain transformers in 3 bullet points.\"],\n",
        "    )\n",
        "    agent_card = AgentCard(\n",
        "        name=\"Google_A2A_Project_Create_OpenAI_Agent\",\n",
        "        description=\"An A2A-compliant agent backed by OpenAI.\",\n",
        "        url=f\"http://{host}:{port}/\",\n",
        "        version=\"1.0.0\",\n",
        "        default_input_modes=[\"text\"],\n",
        "        default_output_modes=[\"text\"],\n",
        "        capabilities=AgentCapabilities(streaming=True),\n",
        "        skills=[skill],\n",
        "    )\n",
        "    handler = DefaultRequestHandler(\n",
        "        agent_executor=MyAgentTaskManager(openai_model=openai_model),\n",
        "        task_store=InMemoryTaskStore(),\n",
        "    )\n",
        "    app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)\n",
        "    uvicorn.run(app.build(), host=host, port=port)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--host\", default=os.getenv(\"HOST\", \"127.0.0.1\"))\n",
        "    parser.add_argument(\"--port\", type=int, default=int(os.getenv(\"PORT\", \"9999\")))\n",
        "    parser.add_argument(\"--openai-model\", default=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"))\n",
        "    args = parser.parse_args()\n",
        "    main(args.host, args.port, args.openai_model)\n",
        "'''\n",
        "open(os.path.join(base_dir, '__init__.py'), 'w').write(init_py)\n",
        "print('Wrote', os.path.join(base_dir, '__init__.py'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/my_project/__init__.py\n"
          ]
        }
      ],
      "execution_count": 17,
      "id": "write_init_py"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "launcher_bg"
      },
      "source": [
        "# Runs uvicorn in a background thread and waits until the port is open\n",
        "import asyncio, socket, threading, time\n",
        "import uvicorn\n",
        "\n",
        "from a2a.server.apps import A2AStarletteApplication\n",
        "from a2a.server.request_handlers import DefaultRequestHandler\n",
        "from a2a.server.tasks import InMemoryTaskStore\n",
        "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
        "from my_project.task_manager import MyAgentTaskManager\n",
        "\n",
        "_server = None\n",
        "_server_thread = None\n",
        "\n",
        "def _is_port_free(host: str, port: int) -> bool:\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.2)\n",
        "        return s.connect_ex((host, port)) != 0\n",
        "\n",
        "def _find_free_port(host: str = \"127.0.0.1\", preferred: int = 9999, max_tries: int = 50) -> int:\n",
        "    if _is_port_free(host, preferred):\n",
        "        return preferred\n",
        "    for p in range(preferred + 1, preferred + 1 + max_tries):\n",
        "        if _is_port_free(host, p):\n",
        "            return p\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind((host, 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "def start_a2a_server(host: str = \"127.0.0.1\", preferred_port: int = 9999, openai_model: str = \"gpt-4o-mini\"):\n",
        "    global _server, _server_thread\n",
        "    port = _find_free_port(host, preferred_port)\n",
        "\n",
        "    skill = AgentSkill(\n",
        "        id=\"general_chat\",\n",
        "        name=\"General Chat\",\n",
        "        description=\"Answers questions with helpful, concise responses.\",\n",
        "        tags=[\"chat\", \"assistant\"],\n",
        "        examples=[\"Explain transformers in 3 bullet points.\"],\n",
        "    )\n",
        "    agent_card = AgentCard(\n",
        "        name=\"Google_A2A_Project_Create_OpenAI_Agent\",\n",
        "        description=\"An A2A-compliant agent backed by OpenAI.\",\n",
        "        url=f\"http://{host}:{port}/\",\n",
        "        version=\"1.0.0\",\n",
        "        default_input_modes=[\"text\"],\n",
        "        default_output_modes=[\"text\"],\n",
        "        capabilities=AgentCapabilities(streaming=True),\n",
        "        skills=[skill],\n",
        "    )\n",
        "\n",
        "    handler = DefaultRequestHandler(\n",
        "        agent_executor=MyAgentTaskManager(openai_model=openai_model),\n",
        "        task_store=InMemoryTaskStore(),\n",
        "    )\n",
        "    app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler).build()\n",
        "    config = uvicorn.Config(app, host=host, port=port, log_level=\"info\")\n",
        "    _server = uvicorn.Server(config)\n",
        "\n",
        "    def _serve():\n",
        "        asyncio.run(_server.serve())\n",
        "\n",
        "    _server_thread = threading.Thread(target=_serve, daemon=True)\n",
        "    _server_thread.start()\n",
        "\n",
        "    # Wait for socket to open\n",
        "    t0 = time.time()\n",
        "    while time.time() - t0 < 6.0:\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.settimeout(0.2)\n",
        "            if s.connect_ex((host, port)) == 0:\n",
        "                print(f\"A2A server is up at http://{host}:{port}\")\n",
        "                return f\"http://{host}:{port}\"\n",
        "        time.sleep(0.1)\n",
        "    raise RuntimeError(\"Server did not bind in time\")\n",
        "\n",
        "def stop_a2a_server():\n",
        "    global _server\n",
        "    if _server is not None:\n",
        "        _server.should_exit = True\n",
        "        print(\"Requested server shutdown.\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "id": "launcher_bg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "start_server",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "204a01f6-7a29-4007-d9c9-5b5e570e91bd"
      },
      "source": [
        "BASE = start_a2a_server(host=\"127.0.0.1\", preferred_port=9999, openai_model=\"gpt-4o-mini\")\n",
        "BASE"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [447]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:9999 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A2A server is up at http://127.0.0.1:9999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://127.0.0.1:9999'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "execution_count": 19,
      "id": "start_server"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_agent_card",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb6eb61-4ba1-47da-cea4-47c7a0403665"
      },
      "source": [
        "import httpx, json\n",
        "r = httpx.get(f\"{BASE}/.well-known/agent.json\", timeout=10)\n",
        "print(r.status_code)\n",
        "print(json.dumps(r.json(), indent=2)[:800])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:a2a.server.apps.jsonrpc.jsonrpc_app:Deprecated agent card endpoint '/.well-known/agent.json' accessed. Please use '/.well-known/agent-card.json' instead. This endpoint will be removed in a future version.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     127.0.0.1:53496 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n",
            "200\n",
            "{\n",
            "  \"capabilities\": {\n",
            "    \"streaming\": true\n",
            "  },\n",
            "  \"defaultInputModes\": [\n",
            "    \"text\"\n",
            "  ],\n",
            "  \"defaultOutputModes\": [\n",
            "    \"text\"\n",
            "  ],\n",
            "  \"description\": \"An A2A-compliant agent backed by OpenAI.\",\n",
            "  \"name\": \"Google_A2A_Project_Create_OpenAI_Agent\",\n",
            "  \"preferredTransport\": \"JSONRPC\",\n",
            "  \"protocolVersion\": \"0.3.0\",\n",
            "  \"skills\": [\n",
            "    {\n",
            "      \"description\": \"Answers questions with helpful, concise responses.\",\n",
            "      \"examples\": [\n",
            "        \"Explain transformers in 3 bullet points.\"\n",
            "      ],\n",
            "      \"id\": \"general_chat\",\n",
            "      \"name\": \"General Chat\",\n",
            "      \"tags\": [\n",
            "        \"chat\",\n",
            "        \"assistant\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"url\": \"http://127.0.0.1:9999/\",\n",
            "  \"version\": \"1.0.0\"\n",
            "}\n"
          ]
        }
      ],
      "execution_count": 20,
      "id": "test_agent_card"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_send_task",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf06b9f2-f503-486c-89e1-f65d2a273942"
      },
      "source": [
        "import httpx, uuid, json\n",
        "\n",
        "BASE = BASE  # whatever you printed from start_a2a_server()\n",
        "\n",
        "task_id = str(uuid.uuid4())\n",
        "message_id = str(uuid.uuid4())  # REQUIRED in this SDK version\n",
        "\n",
        "payload = {\n",
        "  \"jsonrpc\": \"2.0\",\n",
        "  \"id\": \"1\",\n",
        "  \"method\": \"message/send\",      # modern method name\n",
        "  \"params\": {\n",
        "    \"id\": task_id,\n",
        "    \"contextId\": \"ctx-001\",\n",
        "    \"message\": {\n",
        "      \"messageId\": message_id,   # <-- add this\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        {\"type\": \"text\", \"text\": \"Say hello in 8 words.\"}\n",
        "      ]\n",
        "    },\n",
        "    \"metadata\": {}\n",
        "  }\n",
        "}\n",
        "\n",
        "r = httpx.post(f\"{BASE}/\", json=payload, timeout=60)\n",
        "print(r.status_code, r.headers.get(\"content-type\"))\n",
        "print(json.dumps(r.json(), indent=2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     127.0.0.1:33042 - \"POST / HTTP/1.1\" 200 OK\n",
            "200 application/json\n",
            "{\n",
            "  \"id\": \"1\",\n",
            "  \"jsonrpc\": \"2.0\",\n",
            "  \"result\": {\n",
            "    \"contextId\": \"ee239b70-9ccd-48ee-ad68-0a24e1e1f22c\",\n",
            "    \"kind\": \"message\",\n",
            "    \"messageId\": \"f6bb1dc1-53e0-411b-b438-8816688e449e\",\n",
            "    \"parts\": [\n",
            "      {\n",
            "        \"kind\": \"text\",\n",
            "        \"text\": \"Hello! How are you doing today, my friend?\"\n",
            "      }\n",
            "    ],\n",
            "    \"role\": \"agent\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "execution_count": 23,
      "id": "test_send_task"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stop_server",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ff799c-1640-4b21-f313-b3608ff82c37"
      },
      "source": [
        "stop_a2a_server()\n",
        "print('Stopped.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requested server shutdown.\n",
            "Stopped.\n"
          ]
        }
      ],
      "execution_count": 24,
      "id": "stop_server"
    }
  ]
}