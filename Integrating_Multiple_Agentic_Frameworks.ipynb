{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSNp8BlLQ9QKnsgvi4EKW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-adeleke1/Association_of_Data_Scientists/blob/main/Integrating_Multiple_Agentic_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integrating Multiple Agentic Frameworks\n",
        "\n",
        "In this tutorial, we explore how to combine LangGraph, a flexible framework for building agentic workflows, with AutoGen, a powerful agent system from Microsoft, to build an intelligent chatbot that:\n",
        "\n",
        "Supports persistent memory between interactions\n",
        "\n",
        "Uses AutoGen's reasoning abilities to handle complex tasks\n",
        "\n",
        "Leverages LangGraph's features like task routing, entrypoints, and streaming output\n",
        "\n",
        "##Why integrate LangGraph with other frameworks?\n",
        "\n",
        "LangGraph provides strong abstractions for stateful, multi-step applications. Integrating external agent frameworks like AutoGen enables:\n",
        "\n",
        "Modular agent design across ecosystems\n",
        "Seamless use of LangGraph features (memory, streaming, persistence)\n",
        "Support for hybrid multi-agent workflows\n"
      ],
      "metadata": {
        "id": "Ufz4oI7jJND-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "Yzbd1ya3JfBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrZAN-XXJBH0",
        "outputId": "3cbb1349-ad9a-4162-8269-80cbe43a951a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.9.9-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ag2==0.9.9 (from autogen)\n",
            "  Downloading ag2-0.9.9-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (4.10.0)\n",
            "Collecting asyncer==0.0.8 (from ag2==0.9.9->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from ag2==0.9.9->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from ag2==0.9.9->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (1.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ag2==0.9.9->autogen) (0.11.0)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.74)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.9->autogen) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.9->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.9->autogen) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.9->autogen) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.9->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.9->autogen) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker->ag2==0.9.9->autogen) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->ag2==0.9.9->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Downloading autogen-0.9.9-py3-none-any.whl (13 kB)\n",
            "Downloading ag2-0.9.9-py3-none-any.whl (833 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.0/834.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, diskcache, docker, asyncer, langgraph-sdk, ag2, autogen, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed ag2-0.9.9 asyncer-0.0.8 autogen-0.9.9 diskcache-5.6.3 docker-7.1.0 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.3 ormsgpack-1.10.0\n"
          ]
        }
      ],
      "source": [
        "%pip install autogen langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Configure OpenAI Key"
      ],
      "metadata": {
        "id": "7FLvGfwlJo_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yDo_Q6XJp_l",
        "outputId": "031622e2-6433-4888-dcf8-b491a2bf3a70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Define AutoGen Agents"
      ],
      "metadata": {
        "id": "93UMVX3rJ3kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use AutoGen's AssistantAgent and UserProxyAgent. These agents can auto-respond and run code when needed."
      ],
      "metadata": {
        "id": "bZKGuA1eJ8TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "import os\n",
        "\n",
        "config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n",
        "\n",
        "llm_config = {\n",
        "    \"timeout\": 600,\n",
        "    \"cache_seed\": 42,\n",
        "    \"config_list\": config_list,\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "autogen_agent = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\n",
        "        \"work_dir\": \"web\",\n",
        "        \"use_docker\": False,\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n",
        ")"
      ],
      "metadata": {
        "id": "7OmKgvBCJ4se"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Create the LangGraph Workflow"
      ],
      "metadata": {
        "id": "FUWJa-fsKLU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s build a LangGraph graph that delegates reasoning to AutoGen, with support for short-term memory and checkpointing.\n"
      ],
      "metadata": {
        "id": "kc9fbfRFKQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages, BaseMessage\n",
        "from langgraph.func import entrypoint, task\n",
        "from langgraph.graph import add_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "@task\n",
        "def call_autogen_agent(messages: list[BaseMessage]):\n",
        "    # convert to openai-style messages\n",
        "    messages = convert_to_openai_messages(messages)\n",
        "    response = user_proxy.initiate_chat(\n",
        "        autogen_agent,\n",
        "        message=messages[-1],\n",
        "        # pass previous message history as context\n",
        "        carryover=messages[:-1],\n",
        "    )\n",
        "    # get the final response from the agent\n",
        "    content = response.chat_history[-1][\"content\"]\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "\n",
        "# add short-term memory for storing conversation history\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "\n",
        "@entrypoint(checkpointer=checkpointer)\n",
        "def workflow(messages: list[BaseMessage], previous: list[BaseMessage]):\n",
        "    messages = add_messages(previous or [], messages)\n",
        "    response = call_autogen_agent(messages).result()\n",
        "    return entrypoint.final(value=response, save=add_messages(messages, response))"
      ],
      "metadata": {
        "id": "a6d8DISpKM6q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Step 5: Run the Graph\n",
        "\n",
        "Initialize a Config with Thread ID"
      ],
      "metadata": {
        "id": "MYAVpIguKZv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the thread ID to persist agent outputs for future interactions\n",
        "# highlight-next-line\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "for chunk in workflow.stream(\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Which numbers between 1 and 50 are divisible by 7?\",\n",
        "        }\n",
        "    ],\n",
        "    # highlight-next-line\n",
        "    config,\n",
        "):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWJgdkGsKbEF",
        "outputId": "1cbffcc9-b880-4792-d4cb-be26a21b9283"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Which numbers between 1 and 50 are divisible by 7?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "To find the numbers between 1 and 50 that are divisible by 7, we can iterate through the numbers in this range and check if each number is divisible by 7. Here's a simple Python script to do that:\n",
            "\n",
            "```python\n",
            "# filename: divisible_by_7.py\n",
            "\n",
            "def find_divisible_by_7():\n",
            "    divisible_numbers = [number for number in range(1, 51) if number % 7 == 0]\n",
            "    print(\"Numbers between 1 and 50 that are divisible by 7:\", divisible_numbers)\n",
            "\n",
            "find_divisible_by_7()\n",
            "```\n",
            "\n",
            "This script will output the numbers between 1 and 50 that are divisible by 7. Please execute the code to see the result.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Numbers between 1 and 50 that are divisible by 7: [7, 14, 21, 28, 35, 42, 49]\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "The numbers between 1 and 50 that are divisible by 7 are: 7, 14, 21, 28, 35, 42, and 49. \n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (6b602509-4512-4915-8405-c6ad7f2ded04): Termination message condition on agent 'user_proxy' met\n",
            "{'call_autogen_agent': {'role': 'assistant', 'content': 'The numbers between 1 and 50 that are divisible by 7 are: 7, 14, 21, 28, 35, 42, and 49. \\n\\nTERMINATE'}}\n",
            "{'workflow': {'role': 'assistant', 'content': 'The numbers between 1 and 50 that are divisible by 7 are: 7, 14, 21, 28, 35, 42, and 49. \\n\\nTERMINATE'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Continue the Conversation**\n",
        "Because we're using LangGraph’s persistence layer, we can pick up the thread in future runs:\n"
      ],
      "metadata": {
        "id": "kb73FzcDK6y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in workflow.stream(\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Multiply the last number by 3\",\n",
        "        }\n",
        "    ],\n",
        "    # highlight-next-line\n",
        "    config,\n",
        "):\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQaxqDxCK7o4",
        "outputId": "c447e817-dc03-432c-fb73-89167094b1dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Multiply the last number by 3\n",
            "Context: \n",
            "Which numbers between 1 and 50 are divisible by 7?\n",
            "The numbers between 1 and 50 that are divisible by 7 are: 7, 14, 21, 28, 35, 42, and 49. \n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "The last number in the list of numbers between 1 and 50 that are divisible by 7 is 49. To multiply this number by 3:\n",
            "\n",
            "49 * 3 = 147\n",
            "\n",
            "Therefore, the result is 147.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (39dab331-cbda-447a-9e2a-f50ec0a57c36): Termination message condition on agent 'user_proxy' met\n",
            "{'call_autogen_agent': {'role': 'assistant', 'content': 'The last number in the list of numbers between 1 and 50 that are divisible by 7 is 49. To multiply this number by 3:\\n\\n49 * 3 = 147\\n\\nTherefore, the result is 147.\\n\\nTERMINATE'}}\n",
            "{'workflow': {'role': 'assistant', 'content': 'The last number in the list of numbers between 1 and 50 that are divisible by 7 is 49. To multiply this number by 3:\\n\\n49 * 3 = 147\\n\\nTherefore, the result is 147.\\n\\nTERMINATE'}}\n"
          ]
        }
      ]
    }
  ]
}