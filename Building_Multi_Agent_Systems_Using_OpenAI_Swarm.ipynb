{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-adeleke1/Association_of_Data_Scientists/blob/main/Building_Multi_Agent_Systems_Using_OpenAI_Swarm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/swarm.git"
      ],
      "metadata": {
        "id": "ZZ8SnJluMBiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6970c891-898e-4a4a-b63e-bb82022a3c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-mldet6v6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-mldet6v6\n",
            "  Resolved https://github.com/openai/swarm.git to commit 0c82d7d868bb8e2d380dfd2a319b5c3a1f4c0cb9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from swarm==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from swarm==0.1.0) (1.100.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from swarm==0.1.0) (8.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from swarm==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from swarm==0.1.0) (4.67.1)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.3.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.14.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (3.12.15)\n",
            "Collecting diskcache>=5.6.3 (from instructor->swarm==0.1.0)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (0.17.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.7.0 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from instructor->swarm==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->swarm==0.1.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->swarm==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->swarm==0.1.0) (2025.8.3)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.13-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->swarm==0.1.0) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->swarm==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->swarm==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.20.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor->swarm==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.19.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.10.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.3.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.0/221.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading identify-2.6.13-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.34.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25913 sha256=52c8d482fb7e8858d85f57bf35ccdd13460617e45cde8de07d13e8ad74311aab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lia7kf1x/wheels/7e/5a/c8/4a3701c58e2a546c7be4c838027b5d2f2da5bff63ff1a8c8ee\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, identify, diskcache, cfgv, pre-commit, instructor, swarm\n",
            "Successfully installed cfgv-3.4.0 diskcache-5.6.3 distlib-0.4.0 identify-2.6.13 instructor-1.10.0 nodeenv-1.9.1 pre-commit-4.3.0 swarm-0.1.0 virtualenv-20.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai firecrawl-py serpapi google-search-results"
      ],
      "metadata": {
        "id": "83J1LSWPLVKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3f1af5-1a54-4c2f-dfa4-71a700032007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-3.3.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting serpapi\n",
            "  Downloading serpapi-0.1.5-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (1.1.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (15.0.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (3.12.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.20.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->firecrawl-py) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->firecrawl-py) (2.5.0)\n",
            "Downloading firecrawl_py-3.3.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading serpapi-0.1.5-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=b1cf743047d8ccd0a2908e2f856db15460e646b4822c3ec64ba0a5b29b70bb70\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/47/f5/89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: serpapi, google-search-results, firecrawl-py\n",
            "Successfully installed firecrawl-py-3.3.2 google-search-results-2.4.2 serpapi-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfobA_VRI9RA",
        "outputId": "a85f6960-c864-4ed0-b894-e1db0c38115f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Agent\n",
        "from swarm.repl import run_demo_loop\n",
        "import dotenv\n",
        "from serpapi import GoogleSearch\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the env having API Keys - OpenAI, FireCrawl and SerpAPI\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Initialize FirecrawlApp and OpenAI\n",
        "app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "#app = FirecrawlApp(api_key=userdata.get(\"FIRECRAWL_API_KEY\"))\n",
        "#client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Defining functions for agents\n",
        "def search_google(query, objective):\n",
        "    \"\"\"Search Google using SerpAPI.\"\"\"\n",
        "    print(f\"Parameters: query={query}, objective={objective}\")\n",
        "    search = GoogleSearch({\"q\": query, \"api_key\": os.getenv(\"SERP_API_KEY\")})\n",
        "    #search = GoogleSearch({\"q\": query, \"api_key\": userdata.get(\"SERP_API_KEY\")})\n",
        "    results = search.get_dict().get(\"organic_results\", [])\n",
        "    return {\"objective\": objective, \"results\": results}\n",
        "\n",
        "def map_url_pages(url, objective):\n",
        "    \"\"\"Map a website's pages using Firecrawl.\"\"\"\n",
        "    search_query = generate_completion(\n",
        "        \"website search query generator\",\n",
        "        f\"Generate a 1-2 word search query for the website: {url} based on the objective\",\n",
        "        \"Objective: \" + objective\n",
        "    )\n",
        "    print(f\"Parameters: url={url}, objective={objective}, search_query={search_query}\")\n",
        "    #map_status = app.map(url, params={'search': search_query})\n",
        "    map_status = app.map(url=url, search=search_query)\n",
        "    if map_status.get('status') == 'success':\n",
        "        links = map_status.get('links', [])\n",
        "        top_link = links[0] if links else None\n",
        "        return {\"objective\": objective, \"results\": [top_link] if top_link else []}\n",
        "    else:\n",
        "        return {\"objective\": objective, \"results\": []}\n",
        "\n",
        "def scrape_url(url, objective):\n",
        "    \"\"\"Scrape a website using Firecrawl.\"\"\"\n",
        "    print(f\"Parameters: url={url}, objective={objective}\")\n",
        "    scrape_status = app.scrape_url(\n",
        "        url,\n",
        "        params={'formats': ['markdown']}\n",
        "    )\n",
        "    return {\"objective\": objective, \"results\": scrape_status}\n",
        "\n",
        "def analyze_website_content(content, objective):\n",
        "    \"\"\"Analyze the scraped website content using OpenAI.\"\"\"\n",
        "    print(f\"Parameters: content={content[:50]}..., objective={objective}\")\n",
        "    analysis = generate_completion(\n",
        "        \"website data extractor\",\n",
        "        f\"Analyze the following website content and extract a JSON object based on the objective.\",\n",
        "        \"Objective: \" + objective + \"\\nContent: \" + content\n",
        "    )\n",
        "    return {\"objective\": objective, \"results\": analysis}\n",
        "\n",
        "def generate_completion(role, task, content):\n",
        "    \"\"\"Generate a completion using OpenAI.\"\"\"\n",
        "    print(f\"Parameters: role={role}, task={task[:50]}..., content={content[:50]}...\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {role}. {task}\"},\n",
        "            {\"role\": \"user\", \"content\": content}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Defining handoffs for context variable updations\n",
        "def handoff_to_search_google():\n",
        "    \"\"\"Hand off the search query to the search google agent.\"\"\"\n",
        "    return google_search_agent\n",
        "\n",
        "def handoff_to_map_url():\n",
        "    \"\"\"Hand off the url to the map url agent.\"\"\"\n",
        "    return map_url_agent\n",
        "\n",
        "def handoff_to_website_scraper():\n",
        "    \"\"\"Hand off the url to the website scraper agent.\"\"\"\n",
        "    return website_scraper_agent\n",
        "\n",
        "def handoff_to_analyst():\n",
        "    \"\"\"Hand off the website content to the analyst agent.\"\"\"\n",
        "    return analyst_agent\n",
        "\n",
        "# Defining Agents\n",
        "# UI agent for user-interaction\n",
        "user_interface_agent = Agent(\n",
        "    name=\"User Interface Agent\",\n",
        "    instructions=\"You are a user interface agent that handles all interactions with the user. You need to always start with an web data extraction objective that the user wants to achieve by searching the web, mapping the web pages, and extracting the content from a specific page. Be concise.\",\n",
        "    functions=[handoff_to_search_google],\n",
        ")\n",
        "\n",
        "# Google search agent for searching web\n",
        "google_search_agent = Agent(\n",
        "    name=\"Google Search Agent\",\n",
        "    instructions=\"You are a google search agent specialized in searching the web. Only search for the website not any specific page. When you are done, you must hand off to the map agent.\",\n",
        "    functions=[search_google, handoff_to_map_url],\n",
        ")\n",
        "\n",
        "# URL mapping agent for mapping web pages\n",
        "map_url_agent = Agent(\n",
        "    name=\"Map URL Agent\",\n",
        "    instructions=\"You are a map url agent specialized in mapping the web pages. When you are done, you must hand off the results to the website scraper agent.\",\n",
        "    functions=[map_url_pages, handoff_to_website_scraper],\n",
        ")\n",
        "\n",
        "# Website scraper agent for scraping data off the website\n",
        "website_scraper_agent = Agent(\n",
        "    name=\"Website Scraper Agent\",\n",
        "    instructions=\"You are a website scraper agent specialized in scraping website content. When you are done, you must hand off the website content to the analyst agent to extract the data based on the objective.\",\n",
        "    functions=[scrape_url, handoff_to_analyst],\n",
        ")\n",
        "\n",
        "# Analyst agent for understanding the website content and displaying in JSON format\n",
        "analyst_agent = Agent(\n",
        "    name=\"Analyst Agent\",\n",
        "    instructions=\"You are an analyst agent that examines website content and returns a JSON object. When you are done, you must return a JSON object.\",\n",
        "    functions=[analyze_website_content],\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_demo_loop(user_interface_agent, stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "# Importing the necessary libraries\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Dict, List, Optional\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "\n",
        "# --- Firecrawl imports (support v2, v1, or legacy clients) ---\n",
        "Firecrawl = None\n",
        "FirecrawlClient = None\n",
        "FirecrawlApp = None\n",
        "try:\n",
        "    # v2 (most recent)\n",
        "    from firecrawl import Firecrawl as _FCV2\n",
        "    Firecrawl = _FCV2\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    # some v2 installs export FirecrawlClient\n",
        "    from firecrawl import FirecrawlClient as _FCClient\n",
        "    FirecrawlClient = _FCClient\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    # v1\n",
        "    from firecrawl import FirecrawlApp as _FCV1\n",
        "    FirecrawlApp = _FCV1\n",
        "except Exception:\n",
        "    try:\n",
        "        from firecrawl.firecrawl import FirecrawlApp as _FCV1b\n",
        "        FirecrawlApp = _FCV1b\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "from swarm import Agent\n",
        "from swarm.repl import run_demo_loop\n",
        "import dotenv\n",
        "from serpapi import GoogleSearch\n",
        "from openai import OpenAI\n",
        "\n",
        "# Colab-only import: make safe outside Colab\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "except Exception:\n",
        "    userdata = None\n",
        "\n",
        "# Load env containing API Keys - OpenAI, FireCrawl and SerpAPI\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "def _get_env(name: str) -> Optional[str]:\n",
        "    \"\"\"Read env var; if on Colab and set in userdata, prefer that as fallback.\"\"\"\n",
        "    val = os.getenv(name)\n",
        "    if (not val) and userdata:\n",
        "        try:\n",
        "            val = userdata.get(name)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return val\n",
        "\n",
        "_FC_API_KEY = _get_env(\"FIRECRAWL_API_KEY\")\n",
        "_OAI_API_KEY = _get_env(\"OPENAI_API_KEY\")\n",
        "_SERP_KEY = _get_env(\"SERP_API_KEY\")\n",
        "\n",
        "# Build a Firecrawl client compatible with v1/v2\n",
        "def build_firecrawl_client() -> Any:\n",
        "    if Firecrawl is not None:\n",
        "        return Firecrawl(api_key=_FC_API_KEY)            # v2\n",
        "    if FirecrawlClient is not None:\n",
        "        return FirecrawlClient(api_key=_FC_API_KEY)      # v2 (alt export)\n",
        "    if FirecrawlApp is not None:\n",
        "        return FirecrawlApp(api_key=_FC_API_KEY)         # v1\n",
        "    raise ImportError(\"No compatible Firecrawl client found. Please install/upgrade `firecrawl`.\")\n",
        "\n",
        "app = build_firecrawl_client()\n",
        "client = OpenAI(api_key=_OAI_API_KEY)\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "\n",
        "def safe_preview(obj: Any, n: int = 50) -> str:\n",
        "    s = obj\n",
        "    try:\n",
        "        if not isinstance(s, str):\n",
        "            s = json.dumps(s) if isinstance(s, (dict, list)) else str(s)\n",
        "    except Exception:\n",
        "        s = str(obj)\n",
        "    return (s[:n] + \"...\") if len(s) > n else s\n",
        "\n",
        "def normalize_homepage(u: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Given a string that may be a URL, return a normalized homepage URL\n",
        "    (scheme://netloc/). Returns None if not parseable.\n",
        "    \"\"\"\n",
        "    if not u or not isinstance(u, str):\n",
        "        return None\n",
        "    u = u.strip()\n",
        "    # If user typed domain without scheme, try https\n",
        "    if u.startswith(\"http://\") or u.startswith(\"https://\"):\n",
        "        parsed = urlparse(u)\n",
        "    else:\n",
        "        parsed = urlparse(\"https://\" + u)\n",
        "    if not parsed.netloc:\n",
        "        return None\n",
        "    normalized = urlunparse((parsed.scheme or \"https\", parsed.netloc, \"/\", \"\", \"\", \"\"))\n",
        "    return normalized\n",
        "\n",
        "def pick_homepage_from_serp(serp_results: Any) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    From SerpAPI 'organic_results', pick a likely homepage\n",
        "    (first result's root, preferring domain roots).\n",
        "    \"\"\"\n",
        "    if not isinstance(serp_results, list):\n",
        "        return None\n",
        "    candidates: List[str] = []\n",
        "    for item in serp_results:\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "        link = item.get(\"link\")\n",
        "        if isinstance(link, str):\n",
        "            candidates.append(link)\n",
        "    best = None\n",
        "    best_score = 10**9\n",
        "    for link in candidates:\n",
        "        parsed = urlparse(link if link.startswith(\"http\") else \"https://\" + link)\n",
        "        if not parsed.netloc:\n",
        "            continue\n",
        "        path_len = 0 if parsed.path in (\"\", \"/\") else len(parsed.path.strip(\"/\").split(\"/\"))\n",
        "        score = (path_len, len(parsed.netloc))  # tie-breaker: shorter host\n",
        "        if score < best_score:\n",
        "            best = urlunparse((parsed.scheme or \"https\", parsed.netloc, \"/\", \"\", \"\", \"\"))\n",
        "            best_score = score\n",
        "    return best\n",
        "\n",
        "def extract_text_from_scrape_payload(payload: Any) -> str:\n",
        "    \"\"\"Try to extract textual content from whatever the scraper returned.\"\"\"\n",
        "    if payload is None:\n",
        "        return \"\"\n",
        "    if isinstance(payload, str):\n",
        "        return payload\n",
        "    if isinstance(payload, dict):\n",
        "        for key in (\"markdown\", \"content\", \"text\", \"data\", \"pageContent\", \"html\"):\n",
        "            if key in payload and isinstance(payload[key], str):\n",
        "                return payload[key]\n",
        "        try:\n",
        "            return json.dumps(payload)\n",
        "        except Exception:\n",
        "            return str(payload)\n",
        "    try:\n",
        "        return json.dumps(payload)\n",
        "    except Exception:\n",
        "        return str(payload)\n",
        "\n",
        "def _is_homepage_request(*texts: Optional[str]) -> bool:\n",
        "    \"\"\"\n",
        "    Return True if any provided text suggests the user wants the homepage.\n",
        "    \"\"\"\n",
        "    keys = (\"home\", \"homepage\", \"home page\", \"site root\", \"root page\")\n",
        "    for t in texts:\n",
        "        if isinstance(t, str) and any(k in t.lower() for k in keys):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# -----------------------\n",
        "# Firecrawl compatibility\n",
        "# -----------------------\n",
        "\n",
        "def firecrawl_map(client_obj: Any, url: str, search_query: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Call the correct Firecrawl method to map a site, abstracting v1/v2 differences.\n",
        "    Returns a dict with: {'status': 'success'|'error', 'links': [...]}\n",
        "    \"\"\"\n",
        "    # v2: .map(url=..., search=...)\n",
        "    if hasattr(client_obj, \"map\"):\n",
        "        result = client_obj.map(url=url, search=search_query)\n",
        "        links = []\n",
        "        if isinstance(result, dict):\n",
        "            links = result.get(\"links\") or result.get(\"data\") or []\n",
        "        elif isinstance(result, list):\n",
        "            links = result\n",
        "        return {\"status\": \"success\", \"links\": links}\n",
        "\n",
        "    # v1: .map_url(url, params={'search': ...})\n",
        "    if hasattr(client_obj, \"map_url\"):\n",
        "        params = {\"search\": search_query} if search_query else None\n",
        "        result = client_obj.map_url(url, params=params)\n",
        "        links = []\n",
        "        if isinstance(result, dict):\n",
        "            links = result.get(\"links\") or []\n",
        "        elif isinstance(result, list):\n",
        "            links = result\n",
        "        return {\"status\": \"success\", \"links\": links}\n",
        "\n",
        "    return {\"status\": \"error\", \"links\": [], \"error\": \"No map/map_url method on Firecrawl client.\"}\n",
        "\n",
        "def firecrawl_scrape(client_obj: Any, url: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Call the correct Firecrawl method to scrape a single URL, abstracting v1/v2 differences.\n",
        "    Returns a dict; tries to include a 'markdown' key when possible.\n",
        "    \"\"\"\n",
        "    if hasattr(client_obj, \"scrape\"):\n",
        "        out = client_obj.scrape(url=url, formats=[\"markdown\"])\n",
        "        if isinstance(out, dict):\n",
        "            md = out.get(\"markdown\") or out.get(\"content\") or out.get(\"data\") or \"\"\n",
        "            return {\"ok\": True, \"markdown\": md, \"raw\": out}\n",
        "        return {\"ok\": True, \"markdown\": str(out), \"raw\": out}\n",
        "\n",
        "    if hasattr(client_obj, \"scrape_url\"):\n",
        "        out = client_obj.scrape_url(url, params={\"formats\": [\"markdown\"]})\n",
        "        if isinstance(out, dict):\n",
        "            md = out.get(\"markdown\") or out.get(\"content\") or out.get(\"data\") or \"\"\n",
        "            return {\"ok\": True, \"markdown\": md, \"raw\": out}\n",
        "        return {\"ok\": True, \"markdown\": str(out), \"raw\": out}\n",
        "\n",
        "    return {\"ok\": False, \"error\": \"No scrape/scrape_url method on Firecrawl client.\"}\n",
        "\n",
        "# -----------------------\n",
        "# OpenAI wrapper\n",
        "# -----------------------\n",
        "\n",
        "def generate_completion(role: str, task: str, content: str) -> str:\n",
        "    \"\"\"Generate a completion using OpenAI.\"\"\"\n",
        "    print(f\"Parameters: role={role}, task={safe_preview(task)}, content={safe_preview(content)}\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"You are a {role}. {task}\"},\n",
        "            {\"role\": \"user\", \"content\": content}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# -----------------------\n",
        "# Agent tool functions\n",
        "# -----------------------\n",
        "\n",
        "def search_google(query: Optional[str] = None, objective: Optional[str] = None, **kwargs) -> Dict[str, Any]:\n",
        "    \"\"\"Search Google using SerpAPI.\"\"\"\n",
        "    query = query or \"\"\n",
        "    print(f\"Parameters: query={query}, objective={objective}\")\n",
        "    search = GoogleSearch({\"q\": query, \"api_key\": _SERP_KEY})\n",
        "    results = search.get_dict().get(\"organic_results\", []) or []\n",
        "    # bubble up both the query and the raw results for the next tool\n",
        "    return {\"objective\": objective, \"query\": query, \"results\": results}\n",
        "\n",
        "def _derive_site_url(url: Optional[str], objective: Optional[str], results: Optional[Any], **kwargs) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Try multiple strategies to produce a homepage URL:\n",
        "    1) If url provided -> normalize to homepage\n",
        "    2) If objective looks like a URL -> normalize to homepage\n",
        "    3) If Google results present -> pick homepage from SERP\n",
        "    4) If kwargs contains any string field that looks like URL\n",
        "    \"\"\"\n",
        "    # 1) direct url\n",
        "    home = normalize_homepage(url) if url else None\n",
        "    if home:\n",
        "        return home\n",
        "\n",
        "    # 2) objective as URL\n",
        "    if isinstance(objective, str):\n",
        "        home = normalize_homepage(objective)\n",
        "        if home:\n",
        "            return home\n",
        "\n",
        "    # 3) SERP results\n",
        "    home = pick_homepage_from_serp(results)\n",
        "    if home:\n",
        "        return home\n",
        "\n",
        "    # 4) scan kwargs for likely URL strings\n",
        "    for v in kwargs.values():\n",
        "        if isinstance(v, str):\n",
        "            maybe = normalize_homepage(v)\n",
        "            if maybe:\n",
        "                return maybe\n",
        "        if isinstance(v, list):\n",
        "            for item in v:\n",
        "                if isinstance(item, str):\n",
        "                    maybe = normalize_homepage(item)\n",
        "                    if maybe:\n",
        "                        return maybe\n",
        "                if isinstance(item, dict):\n",
        "                    link = item.get(\"link\") or item.get(\"url\")\n",
        "                    maybe = normalize_homepage(link) if isinstance(link, str) else None\n",
        "                    if maybe:\n",
        "                        return maybe\n",
        "    return None\n",
        "\n",
        "def map_url_pages(url: Optional[str] = None,\n",
        "                  objective: Optional[str] = None,\n",
        "                  results: Optional[Any] = None,\n",
        "                  query: Optional[str] = None,\n",
        "                  **kwargs) -> Dict[str, Any]:\n",
        "    \"\"\"Map a website's pages using Firecrawl after deriving the site root.\"\"\"\n",
        "    # Derive a homepage if none was passed\n",
        "    site_url = _derive_site_url(url, objective, results, query=query, **kwargs)\n",
        "    if not site_url:\n",
        "        print(\"map_url_pages: Could not derive a homepage URL; returning empty results.\")\n",
        "        return {\"objective\": objective, \"results\": []}\n",
        "\n",
        "    # If this is clearly a 'homepage' task, skip mapping entirely\n",
        "    if _is_homepage_request(objective, query):\n",
        "        print(f\"map_url_pages: Homepage request detected; returning {site_url} without mapping.\")\n",
        "        return {\"objective\": objective, \"results\": [site_url]}\n",
        "\n",
        "    # Otherwise (non-homepage task), attempt to narrow mapping with a tiny query\n",
        "    search_query = generate_completion(\n",
        "        \"website search query generator\",\n",
        "        f\"Generate a 1-2 word search query for the website: {site_url} based on the objective.\",\n",
        "        \"Objective: \" + (objective or \"\")\n",
        "    )\n",
        "    print(f\"Parameters: url={site_url}, objective={objective}, search_query={safe_preview(search_query)}\")\n",
        "\n",
        "    # Try to map; if Firecrawl rejects the URL or anything goes wrong, fall back to homepage\n",
        "    try:\n",
        "        map_status = firecrawl_map(app, url=site_url, search_query=search_query)\n",
        "        if map_status.get(\"status\") == \"success\":\n",
        "            links = map_status.get(\"links\", []) or []\n",
        "\n",
        "            def _link_of(x):\n",
        "                if isinstance(x, str):\n",
        "                    return x\n",
        "                if isinstance(x, dict):\n",
        "                    return x.get(\"url\") or x.get(\"link\")\n",
        "                return None\n",
        "\n",
        "            for x in links:\n",
        "                lnk = _link_of(x)\n",
        "                if lnk:\n",
        "                    return {\"objective\": objective, \"results\": [lnk]}\n",
        "            # If no links found, still return the homepage\n",
        "            return {\"objective\": objective, \"results\": [site_url]}\n",
        "        else:\n",
        "            print(f\"map_url_pages: Map returned non-success status: {safe_preview(map_status)}\")\n",
        "            return {\"objective\": objective, \"results\": [site_url]}\n",
        "    except Exception as e:\n",
        "        print(f\"map_url_pages: Firecrawl map failed ({type(e).__name__}): {e}\")\n",
        "        return {\"objective\": objective, \"results\": [site_url]}\n",
        "\n",
        "def scrape_url(url: Optional[str] = None,\n",
        "               objective: Optional[str] = None,\n",
        "               results: Optional[Any] = None,\n",
        "               **kwargs) -> Dict[str, Any]:\n",
        "    \"\"\"Scrape a website using Firecrawl.\"\"\"\n",
        "    # If url not provided, try to extract from results\n",
        "    if not url:\n",
        "        # results might be ['https://site/'] or [{'url': ...}]\n",
        "        cand = None\n",
        "        if isinstance(results, list) and results:\n",
        "            if isinstance(results[0], str):\n",
        "                cand = results[0]\n",
        "            elif isinstance(results[0], dict):\n",
        "                cand = results[0].get(\"url\") or results[0].get(\"link\")\n",
        "        if not cand and isinstance(results, dict):\n",
        "            cand = results.get(\"url\") or results.get(\"link\")\n",
        "        url = cand or url\n",
        "    if not url:\n",
        "        print(\"scrape_url: No URL provided or derivable; returning empty payload.\")\n",
        "        return {\"objective\": objective, \"results\": {\"ok\": False, \"error\": \"No URL to scrape.\"}}\n",
        "\n",
        "    print(f\"Parameters: url={url}, objective={objective}\")\n",
        "    scrape_status = firecrawl_scrape(app, url=url)\n",
        "    return {\"objective\": objective, \"results\": scrape_status}\n",
        "\n",
        "def analyze_website_content(\n",
        "    content: Optional[Any] = None,\n",
        "    objective: Optional[str] = None,\n",
        "    results: Optional[Any] = None,\n",
        "    scrape_result: Optional[Any] = None,\n",
        "    **kwargs\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Analyze the scraped website content using OpenAI.\"\"\"\n",
        "    payload = content if content is not None else results if results is not None else scrape_result\n",
        "    text = extract_text_from_scrape_payload(payload)\n",
        "    print(f\"Parameters: content={safe_preview(text)}, objective={objective}\")\n",
        "    analysis = generate_completion(\n",
        "        \"website data extractor\",\n",
        "        \"Analyze the following website content and extract a JSON object based on the objective. Return ONLY valid JSON.\",\n",
        "        \"Objective: \" + (objective or \"\") + \"\\nContent: \" + text\n",
        "    )\n",
        "    return {\"objective\": objective, \"results\": analysis}\n",
        "\n",
        "# -----------------------\n",
        "# Handoffs (accept **kwargs to avoid TypeErrors)\n",
        "# -----------------------\n",
        "\n",
        "def handoff_to_search_google(**kwargs):\n",
        "    \"\"\"Hand off the search query to the Google search agent.\"\"\"\n",
        "    return google_search_agent\n",
        "\n",
        "def handoff_to_map_url(**kwargs):\n",
        "    \"\"\"Hand off the url to the map url agent.\"\"\"\n",
        "    return map_url_agent\n",
        "\n",
        "def handoff_to_website_scraper(**kwargs):\n",
        "    \"\"\"Hand off the url to the website scraper agent.\"\"\"\n",
        "    return website_scraper_agent\n",
        "\n",
        "def handoff_to_analyst(**kwargs):\n",
        "    \"\"\"Hand off the website content to the analyst agent.\"\"\"\n",
        "    return analyst_agent\n",
        "\n",
        "# -----------------------\n",
        "# Agents\n",
        "# -----------------------\n",
        "\n",
        "# UI agent for user-interaction\n",
        "user_interface_agent = Agent(\n",
        "    name=\"User Interface Agent\",\n",
        "    instructions=(\n",
        "        \"You are a user interface agent that handles all interactions with the user. \"\n",
        "        \"You must always start by clarifying the user's web data extraction objective: \"\n",
        "        \"what they want to find, where to look, and what JSON they want returned. \"\n",
        "        \"Then route through: Google Search (to find the site, not a specific page) -> \"\n",
        "        \"Map URL (to list candidate pages) -> Scraper (to fetch a single page) -> \"\n",
        "        \"Analyst (to extract JSON). Be concise.\"\n",
        "    ),\n",
        "    functions=[handoff_to_search_google],\n",
        ")\n",
        "\n",
        "# Google search agent for searching web\n",
        "google_search_agent = Agent(\n",
        "    name=\"Google Search Agent\",\n",
        "    instructions=(\n",
        "        \"You are a Google search agent specialized in searching the web. \"\n",
        "        \"Only search for the website, not any specific page. \"\n",
        "        \"When you are done, you must hand off to the Map URL Agent.\"\n",
        "    ),\n",
        "    functions=[search_google, handoff_to_map_url],\n",
        ")\n",
        "\n",
        "# URL mapping agent for mapping web pages\n",
        "map_url_agent = Agent(\n",
        "    name=\"Map URL Agent\",\n",
        "    instructions=(\n",
        "        \"You are a map url agent specialized in mapping website pages. \"\n",
        "        \"If the user's goal is to 'find the homepage', just return the homepage URL. \"\n",
        "        \"Otherwise, map the site and pick a promising page. \"\n",
        "        \"When you are done, you must hand off the results to the Website Scraper Agent.\"\n",
        "    ),\n",
        "    functions=[map_url_pages, handoff_to_website_scraper],\n",
        ")\n",
        "\n",
        "# Website scraper agent for scraping data off the website\n",
        "website_scraper_agent = Agent(\n",
        "    name=\"Website Scraper Agent\",\n",
        "    instructions=(\n",
        "        \"You are a website scraper agent specialized in scraping website content. \"\n",
        "        \"Scrape only a single selected page. \"\n",
        "        \"When you are done, you must hand off the website content to the Analyst Agent to extract the data \"\n",
        "        \"based on the user's objective.\"\n",
        "    ),\n",
        "    functions=[scrape_url, handoff_to_analyst],\n",
        ")\n",
        "\n",
        "# Analyst agent for understanding the website content and displaying in JSON format\n",
        "analyst_agent = Agent(\n",
        "    name=\"Analyst Agent\",\n",
        "    instructions=(\n",
        "        \"You are an analyst agent that examines website content and returns a JSON object. \"\n",
        "        \"You must return a JSON object and nothing else.\"\n",
        "    ),\n",
        "    functions=[analyze_website_content],\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import inspect\n",
        "        fc_has = {\n",
        "            \"map\": hasattr(app, \"map\"),\n",
        "            \"map_url\": hasattr(app, \"map_url\"),\n",
        "            \"scrape\": hasattr(app, \"scrape\"),\n",
        "            \"scrape_url\": hasattr(app, \"scrape_url\"),\n",
        "        }\n",
        "        print(\"Firecrawl capabilities:\", fc_has)\n",
        "        print(\"handoff_to_map_url signature:\", str(inspect.signature(handoff_to_map_url)))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    run_demo_loop(user_interface_agent, stream=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnNjG9gwdw4y",
        "outputId": "5154615e-15d6-47b9-c66b-a739928cde34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "id": "R2eDDMtxKv1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04103c0f-7a6d-4cbc-b22d-b4eb4feec30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Firecrawl capabilities: {'map': True, 'map_url': False, 'scrape': True, 'scrape_url': False}\n",
            "handoff_to_map_url signature: (**kwargs)\n",
            "Starting Swarm CLI 🐝\n",
            "\u001b[90mUser\u001b[0m: https://adasci.org/mongodb-atlas-vector-search-for-rag-powered-llm-applications/\n",
            "\u001b[94mUser Interface Agent:\u001b[0m To assist you effectively, could you please clarify your objectives for extracting data from this page on adasci.org? What specific information are you looking to find, and how would you like the JSON formatted in the final output?\n",
            "\u001b[90mUser\u001b[0m: Find the homepage of the ADASCI organization. Return in default JSON.\n",
            "\u001b[94mUser Interface Agent: \u001b[95mhandoff_to_search_google\u001b[0m()\n",
            "\u001b[94mGoogle Search Agent: \u001b[95msearch_google\u001b[0m()\n",
            "Parameters: query=ADASCI homepage, objective=None\n",
            "\u001b[94mGoogle Search Agent: \u001b[95mhandoff_to_map_url\u001b[0m()\n",
            "\u001b[94mMap URL Agent: \u001b[95mhandoff_to_website_scraper\u001b[0m()\n",
            "\u001b[94mWebsite Scraper Agent: \u001b[95mscrape_url\u001b[0m()\n",
            "Parameters: url=https://adasci.org/, objective=Find the homepage of the ADASCI organization.\n",
            "\u001b[94mWebsite Scraper Agent: \u001b[95mhandoff_to_analyst\u001b[0m()\n",
            "\u001b[94mAnalyst Agent: \u001b[95manalyze_website_content\u001b[0m()\n",
            "Parameters: content=ADaSci is the leading global professional body for..., objective=Find the homepage of the ADASCI organization.\n",
            "Parameters: role=website data extractor, task=Analyze the following website content and extract ..., content=Objective: Find the homepage of the ADASCI organiz...\n",
            "\u001b[94mAnalyst Agent:\u001b[0m ```json\n",
            "{\n",
            "  \"organization\": \"ADaSci\",\n",
            "  \"homepage\": \"https://www.adasci.org\"\n",
            "}\n",
            "```\n",
            "\u001b[90mUser\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qn-Ty59YMz3_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}